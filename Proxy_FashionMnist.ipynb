{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proxy_FashionMnist_T15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WfIWz-U1Nxc"
      },
      "source": [
        "!pip install spikingjelly\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive')\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNc9eSc_tI1n"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from spikingjelly.clock_driven import neuron, functional, encoding, surrogate, layer\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "_seed_ =  2020\n",
        "torch.manual_seed(_seed_)  # use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(_seed_)\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.static_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128), \n",
        "        )\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),           \n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 14 * 14\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 7 * 7\n",
        "\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, 128 * 4 * 5, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128 * 4 * 5, 128 * 3 * 3, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128 * 3 * 3, 128 * 2 * 1, bias=False),\n",
        "            nn.ReLU (),\n",
        "            nn.Linear(128 * 2 * 1, 10, bias=False),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(self.static_conv(x)))\n",
        "\n",
        "class SNN(nn.Module):\n",
        "    def __init__(self, T, v_threshold=2.0, v_reset=0.0):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.static_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128), \n",
        "        )\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128), \n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "            nn.MaxPool2d(2, 2),  # 14 * 14\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128), \n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "            nn.MaxPool2d(2, 2),  # 7 * 7\n",
        "\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, 128 * 4 * 5, bias=False),\n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "            nn.Linear(128 * 4 * 5, 128 * 3 * 3, bias=False),\n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "            nn.Linear(128 * 3 * 3, 128 * 2 * 1, bias=False),\n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "            nn.Linear(128 * 2 * 1, 10, bias=False),\n",
        "            neuron.IFNode(v_threshold=v_threshold, v_reset=v_reset, surrogate_function=surrogate.ATan()),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.static_conv(x)\n",
        "        out_spikes_counter = self.fc(self.conv(x))\n",
        "        for t in range(1, self.T):\n",
        "            if (t==0):\n",
        "                out_spikes_counter = self.fc(self.conv(x))\n",
        "            else:\n",
        "                out_spikes_counter += self.fc(self.conv(x))\n",
        "\n",
        "        return out_spikes_counter\n",
        "    \n",
        "def main():\n",
        "    #Parameters Setting\n",
        "    device = \"cuda:0\" \n",
        "    dataset_dir = \"./\"\n",
        "    batch_size = 100  \n",
        "    learning_rate = 1e-4\n",
        "    T = 15 \n",
        "    train_epoch = 100\n",
        "    log_dir = \"./\"\n",
        "    model_dir=\"path to ANN and SNN saved models on your local machine or on your Google Drive\"\n",
        "\n",
        "    #Data transormations\n",
        "    test_list_transforms = [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        "    \n",
        "    train_list_transforms = [\n",
        "        transforms.RandomCrop(26),\n",
        "        transforms.Pad(1),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        "    \n",
        "    #Data loaders\n",
        "    train_transform = transforms.Compose(train_list_transforms)\n",
        "    test_transform = transforms.Compose(test_list_transforms)\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=torchvision.datasets.FashionMNIST(\n",
        "            root=dataset_dir,\n",
        "            train=True,\n",
        "            transform=train_transform,\n",
        "            download=True),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        pin_memory=True)\n",
        "    test_data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=torchvision.datasets.FashionMNIST(\n",
        "            root=dataset_dir,\n",
        "            train=False,\n",
        "            transform=test_transform,\n",
        "            download=True),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        pin_memory=True)\n",
        "\n",
        "    \n",
        "    #Building or loading models (ANN and SNN)\n",
        "    print('Load pretrained model? (y/n) ')\n",
        "    pretrained=input()\n",
        "    if pretrained=='y':\n",
        "      print('Loading... ')\n",
        "      ann=torch.load(model_dir+'/ANN_Params.pt',map_location=device)\n",
        "      snn=torch.load(model_dir+'/SNN_Params.pt',map_location=device)\n",
        "      print('Pretrained model loaded!')\n",
        "      print('Evaluation on test data:')\n",
        "      train_epoch=0\n",
        "\n",
        "    else: \n",
        "      ann = ANN().to(device)\n",
        "      snn = SNN(T=T).to(device)\n",
        "      print('Model initialized with random weights!')\n",
        "    \n",
        "       \n",
        "\n",
        "    # Weight Sharing: set ptr of snn's param to point ann's param\n",
        "    params_ann = ann.named_parameters()\n",
        "    params_snn = snn.named_parameters()\n",
        "    dict_params_snn = dict(params_snn)\n",
        "    for name, param in params_ann:\n",
        "        if name in dict_params_snn:\n",
        "            dict_params_snn[name].data = param.data\n",
        "\n",
        "\n",
        "            \n",
        "    #Optimizer Settings        \n",
        "    optimizer_ann = torch.optim.Adam(ann.parameters(), lr=learning_rate, betas=(0.8, 0.99), eps=1e-08, weight_decay=1e-06)\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    #Learning \n",
        "    print('Learning started...')\n",
        "    iterations = 0\n",
        "    for epoch in range(train_epoch):\n",
        "        ann.train()\n",
        "        snn.train()\n",
        "        if epoch>=1:\n",
        "            for m in ann.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eval()\n",
        "            for m in snn.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eval()\n",
        "\n",
        "        correct_ann = 0\n",
        "        correct_snn = 0\n",
        "        sample_num = 0\n",
        "        t_start = time.perf_counter()\n",
        "        for img, label in tqdm(train_data_loader, position=0):\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "            label_one_hot = F.one_hot(label, 10).float()\n",
        "            optimizer_ann.zero_grad()\n",
        "            outputs = ann(img)#ANN output\n",
        "            acc_ann = (outputs.argmax(1) == label).float().mean().item()\n",
        "            with torch.no_grad():\n",
        "                out_spikes_counter = snn(img)#SNN spike counts\n",
        "\n",
        "            #Computing trainig accuracies on each batch\n",
        "            predict_ann = outputs.argmax(1)\n",
        "            correct_ann += (predict_ann == label).sum().item()\n",
        "            sample_num += label.numel()\n",
        "            correct_snn += (out_spikes_counter.argmax(1) == label).float().sum().item()\n",
        "\n",
        "            # out_spikes_counter_frequency =  out_spikes_counter   \n",
        "            outputs.data.copy_(out_spikes_counter)#Replacing SNN output in ANN output layer\n",
        "            loss = F.mse_loss(outputs, label_one_hot)# Comuting the loss in ANN (by the SNN output)\n",
        "            loss.backward()#computing the gradients in ANN\n",
        "            optimizer_ann.step()#updating the shared weights\n",
        "            functional.reset_net(snn)#reseting the snn for next inputs\n",
        "\n",
        "            iterations += 1\n",
        "\n",
        "        acc_ann = correct_ann / sample_num\n",
        "        acc_snn = correct_snn / sample_num        \n",
        "        print(f'epoch={epoch}, train_ann={acc_ann}, train_snn={acc_snn}') #, t_train={t_train}, t_test={t_test}')\n",
        "        t_train = time.perf_counter() - t_start\n",
        "\n",
        "        #Evaluation on test samples\n",
        "        ann.eval()\n",
        "        snn.eval()\n",
        "        t_start = time.perf_counter()          \n",
        "        with torch.no_grad():\n",
        "            correct_snn = 0\n",
        "            correct_ann = 0\n",
        "            sample_num = 0\n",
        "            for img, label in tqdm(test_data_loader, position=0):\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "                predict_ann = ann(img)\n",
        "                correct_ann += (predict_ann.argmax(1) == label).sum().item()\n",
        "                sample_num += label.numel()\n",
        "                out_spikes_counter_frequency = snn(img)\n",
        "                correct_snn += (out_spikes_counter_frequency.argmax(1) == label).sum()\n",
        "\n",
        "                functional.reset_net(snn)\n",
        "\n",
        "            acc_ann = correct_ann / sample_num\n",
        "            acc_snn = correct_snn / sample_num\n",
        "            t_test = time.perf_counter() - t_start\n",
        "            \n",
        "            print(f'epoch={epoch}, acc_ann={acc_ann}, acc_snn={acc_snn}') #, t_train={t_train}, t_test={t_test}')\n",
        "    ann.eval()\n",
        "    snn.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_snn = 0\n",
        "        correct_ann = 0\n",
        "        sample_num = 0\n",
        "        for img, label in tqdm(test_data_loader, position=0):\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "            predict_ann = ann(img)\n",
        "            correct_ann += (predict_ann.argmax(1) == label).sum().item()\n",
        "            sample_num += label.numel()\n",
        "            out_spikes_counter_frequency = snn(img)\n",
        "  \n",
        "            # correct_snn += (out_spikes_counter_frequency.argmax(1) == label).sum()\n",
        "            lab = F.one_hot(label, 10).float()\n",
        "            lab = (lab.cpu()).numpy()\n",
        "            lab = lab.astype(bool)\n",
        "            out_spikes_counter_frequency = ((out_spikes_counter_frequency.cpu()).detach()).numpy()\n",
        "            lab2 = out_spikes_counter_frequency[lab]\n",
        "            correct_snn += (out_spikes_counter_frequency.max(1) == lab2).sum()\n",
        "            \n",
        "            functional.reset_net(snn)\n",
        "\n",
        "        acc_ann = correct_ann / sample_num\n",
        "        acc_snn = correct_snn / sample_num\n",
        "        print(f' Final Result: Acc_ANN={acc_ann}, Acc_SNN={acc_snn}') #, t_train={t_train}, t_test={t_test}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}